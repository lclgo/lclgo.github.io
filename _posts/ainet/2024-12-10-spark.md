---
layout: post
title: "spark"
date: 2024-12-10 08:00:00 +0900
category: basic
---

## 《大数据处理框架Apache Spark设计与实现》阅读笔记

### spark逻辑处理流程 => 物理执行计划

1. 逻辑处理流程
    * 应用程序产生RDD
    * 建立RDD之间的数据依赖关系（宽依赖、窄依赖）
    * RDD数据分区
    * RDD数据计算
2. 根据逻辑处理流程生成物理执行计划
    * 根据action划分job
    * 根据宽依赖划分job为多个执行阶段（stage）
    * 根据分区将stage划分为多个task

物理执行计划生成还需要额外考虑的问题：

1. job、stage和task的计算顺序
2. task内部数据的存储与计算问题：不同类型的task怎么在RDD分区实现内存节省
3. task间的数据传递与计算问题：Shuffle Write + Shuffle Read

### shuffle优化

Shuffle分为Shuffle Write和Shuffle Read两种，前者解决上游stage输出数据的分区问题，后者解决下游stage从上游获取数据、重新组织、并为后续操作提供数据的问题。

* Shuffle Write：可以根据下游task的数量、parent RDD的分区数量确定输出数据的分区数量
* Shuffle Read：可以采用简单的HashMap机制聚合数据

#### 问题和解决方案：

1. 在线数据聚合：对reduceByKey(func)操作，每次操作时都进行数据聚合。
2. Shuffle Write端的combine：采用类似Read端的HashMap操作。
3. sort：采用先聚合在排序的方案。
4. 内存占用优化：在内存不足时，聚合操作会将内存上的数据spill到磁盘上，spill到磁盘上会对数据做排序，优化后续处理。

### codegen

spark基于Jaino动态编译，把SQL语句转换成java语言，通过火山模型把多个算子融合在一起，减少函数调用开销，提升算子执行效率。
